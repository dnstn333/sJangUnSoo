{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnstn\\src\\ch04\n"
     ]
    }
   ],
   "source": [
    "%cd ../src/ch04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 파이썬으로 텍스트 분류하기\n",
    "### 1. 텍스트로 단어 벡터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', \\\n",
    "                  'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', \\\n",
    "                  'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', \\\n",
    "                  'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how',\\\n",
    "                  'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]\n",
    "    return postingList,classVec\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print \"the word: %s is not in my Vocabulary!\" % word\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my']\n"
     ]
    }
   ],
   "source": [
    "import bayes\n",
    "listOPosts, listClasses = bayes.loadDataSet()\n",
    "myVocabList = bayes.createVocabList(listOPosts)\n",
    "print myVocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print bayes.setOfWords2Vec(myVocabList, listOPosts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print bayes.setOfWords2Vec(myVocabList, listOPosts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 단어 벡터로 확률 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = zeros(numWords); p1Num = zeros(numWords)     \n",
    "    p0Denom = 0.0; p1Denom = 0.0                      \n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = p1Num/p1Denom          #change to log()\n",
    "    p0Vect = p0Num/p0Denom          #change to log()\n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOPosts, listClasses = bayes.loadDataSet()\n",
    "myVocabList = bayes.createVocabList(listOPosts)\n",
    "trainMat=[]\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(bayes.setOfWords2Vec(myVocabList,postinDoc))\n",
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)\n",
    "pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04166667,  0.04166667,  0.04166667,  0.        ,  0.        ,\n",
       "        0.04166667,  0.04166667,  0.04166667,  0.        ,  0.04166667,\n",
       "        0.04166667,  0.04166667,  0.04166667,  0.        ,  0.        ,\n",
       "        0.08333333,  0.        ,  0.        ,  0.04166667,  0.        ,\n",
       "        0.04166667,  0.04166667,  0.        ,  0.04166667,  0.04166667,\n",
       "        0.04166667,  0.        ,  0.04166667,  0.        ,  0.04166667,\n",
       "        0.04166667,  0.125     ])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.05263158,  0.05263158,\n",
       "        0.        ,  0.        ,  0.        ,  0.05263158,  0.05263158,\n",
       "        0.        ,  0.        ,  0.        ,  0.05263158,  0.05263158,\n",
       "        0.05263158,  0.05263158,  0.05263158,  0.        ,  0.10526316,\n",
       "        0.        ,  0.05263158,  0.05263158,  0.        ,  0.10526316,\n",
       "        0.        ,  0.15789474,  0.        ,  0.05263158,  0.        ,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 검사 : 실제 조건을 반영하기 위해 분류기 수정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #element-wise mult\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "bayes.testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 준비 : 중복 단어 문서 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 예제 : 스팸 이메일 분류하기\n",
    "### 1. 준비 : 텍스트 토근 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M.L.', 'I', 'have', 'ever', 'laid', 'eyes', 'upon.']\n"
     ]
    }
   ],
   "source": [
    "mySent = 'This book is the best book on Python or M.L. I have ever laid eyes upon.'\n",
    "print mySent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "regEx = re.compile('\\\\W*')\n",
    "listOfTokens = regEx.split(mySent)\n",
    "print listOfTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon']\n"
     ]
    }
   ],
   "source": [
    "print [tok for tok in listOfTokens if len(tok) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'm', 'l', 'i', 'have', 'ever', 'laid', 'eyes', 'upon']\n"
     ]
    }
   ],
   "source": [
    "print [tok.lower() for tok in listOfTokens if len(tok) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emailText = open('email/ham/6.txt').read()\n",
    "listOfTokens = regEx.split(emailText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "\n",
    "def spamTest():\n",
    "    docList=[]; classList=[]; fullText=[]\n",
    "    for i in range(1, 26):\n",
    "        wordList = textParse(open('email/spam/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)\n",
    "    trainingSet = range(50); testSet = []\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    trainMat = []; trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print 'the error rate is: ', float(errorCount)/len(testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification error ['yay', 'you', 'both', 'doing', 'fine', 'working', 'mba', 'design', 'strategy', 'cca', 'top', 'art', 'school', 'new', 'program', 'focusing', 'more', 'right', 'brained', 'creative', 'and', 'strategic', 'approach', 'management', 'the', 'way', 'done', 'today']\n",
      "the error rate is:  0.1\n"
     ]
    }
   ],
   "source": [
    "bayes.spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification error ['yay', 'you', 'both', 'doing', 'fine', 'working', 'mba', 'design', 'strategy', 'cca', 'top', 'art', 'school', 'new', 'program', 'focusing', 'more', 'right', 'brained', 'creative', 'and', 'strategic', 'approach', 'management', 'the', 'way', 'done', 'today']\n",
      "the error rate is:  0.1\n"
     ]
    }
   ],
   "source": [
    "bayes.spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.0\n"
     ]
    }
   ],
   "source": [
    "bayes.spamTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.7 예제 : 나이브 베이스를 사용하여 개인 광고에 포함된 지역 특색 도출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import feedparser\n",
    "ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ny['entries']\n",
    "len(ny['entries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcMostFreq(vocabList,fullText):\n",
    "    import operator\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token]=fullText.count(token)\n",
    "    sortedFreq = sorted(freqDict.iteritems(), key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedFreq[:30]\n",
    "\n",
    "def localWords(feed1,feed0):\n",
    "    import feedparser\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    minLen = min(len(feed1['entries']),len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)\n",
    "    top30Words = calcMostFreq(vocabList,fullText)\n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList: vocabList.remove(pairW[0])\n",
    "    trainingSet = range(2*minLen); testSet=[]\n",
    "    for i in range(20):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != \\\n",
    "            classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print 'the error rate is: ',float(errorCount)/len(testSet)\n",
    "    return vocabList,p0V,p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.6\n"
     ]
    }
   ],
   "source": [
    "ny=feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "sf=feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n",
    "vocabList, p0V, p1V = localWords(ny, sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.55\n"
     ]
    }
   ],
   "source": [
    "vocabList, p0V, p1V = localWords(ny, sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTopWords(ny, sf):\n",
    "    import operator\n",
    "    vocabList, p0V, p1V = localWords(ny, sf)\n",
    "    topNY=[]; topSF=[]\n",
    "    for i in range(len(p0V)):\n",
    "        if p0V[i] > -6.0 : topSF.append((vocabList[i],p0V[i]))\n",
    "        if p1V[i] > -6.0 : topNY.append((vocabList[i],p1V[i]))\n",
    "    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)\n",
    "    print \"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\"\n",
    "    for item in sortedSF:\n",
    "        print item[0]\n",
    "    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)\n",
    "    print \"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY **\"\n",
    "    for item in sortedNY:\n",
    "        print item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.45\n",
      "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\n",
      "married\n",
      "very\n",
      "good\n",
      "any\n",
      "fun\n",
      "people\n",
      "let\n",
      "women\n",
      "work\n",
      "want\n",
      "only\n",
      "chill\n",
      "interested\n",
      "single\n",
      "home\n",
      "hello\n",
      "skate\n",
      "traveling\n",
      "experience\n",
      "music\n",
      "share\n",
      "travel\n",
      "how\n",
      "after\n",
      "talk\n",
      "years\n",
      "interesting\n",
      "day\n",
      "term\n",
      "desi\n",
      "open\n",
      "city\n",
      "girls\n",
      "need\n",
      "meetings\n",
      "find\n",
      "busy\n",
      "going\n",
      "close\n",
      "camping\n",
      "please\n",
      "been\n",
      "gay\n",
      "straight\n",
      "know\n",
      "not\n",
      "has\n",
      "smart\n",
      "discreet\n",
      "lady\n",
      "old\n",
      "back\n",
      "hey\n",
      "friends\n",
      "time\n",
      "all\n",
      "month\n",
      "sleep\n",
      "prop\n",
      "young\n",
      "send\n",
      "advantage\n",
      "sitting\n",
      "minded\n",
      "bestie\n",
      "cool\n",
      "skateboarder\n",
      "cloudy\n",
      "enjoy\n",
      "tired\n",
      "drinks\n",
      "outgoing\n",
      "full\n",
      "here\n",
      "explore\n",
      "healing\n",
      "making\n",
      "great\n",
      "museums\n",
      "smoke\n",
      "massage\n",
      "family\n",
      "dude\n",
      "few\n",
      "wondering\n",
      "company\n",
      "getting\n",
      "effort\n",
      "town\n",
      "join\n",
      "performed\n",
      "india\n",
      "spa\n",
      "needs\n",
      "get\n",
      "sunday\n",
      "birthday\n",
      "coming\n",
      "types\n",
      "man\n",
      "short\n",
      "moments\n",
      "host\n",
      "jack\n",
      "help\n",
      "fit\n",
      "clubbing\n",
      "safe\n",
      "they\n",
      "sucked\n",
      "name\n",
      "sleepless\n",
      "40s\n",
      "mean\n",
      "status\n",
      "harm\n",
      "marital\n",
      "year\n",
      "girl\n",
      "saturday\n",
      "living\n",
      "got\n",
      "receiving\n",
      "ass\n",
      "turning\n",
      "geared\n",
      "attending\n",
      "where\n",
      "area\n",
      "skateboard\n",
      "days\n",
      "times\n",
      "place\n",
      "isn\n",
      "think\n",
      "already\n",
      "platonic\n",
      "one\n",
      "soci\n",
      "another\n",
      "slept\n",
      "least\n",
      "thoughts\n",
      "too\n",
      "white\n",
      "interests\n",
      "relationship\n",
      "boring\n",
      "project\n",
      "cabin\n",
      "sac\n",
      "saf\n",
      "say\n",
      "nights\n",
      "2016\n",
      "towards\n",
      "visa\n",
      "don\n",
      "artistic\n",
      "professional\n",
      "215\n",
      "bedroom\n",
      "pretty\n",
      "equally\n",
      "his\n",
      "dependent\n",
      "bay\n",
      "grab\n",
      "geary\n",
      "skateboarding\n",
      "best\n",
      "enough\n",
      "email\n",
      "complementary\n",
      "attention\n",
      "group\n",
      "etc\n",
      "supply\n",
      "alone\n",
      "likes\n",
      "learning\n",
      "boat\n",
      "decent\n",
      "whom\n",
      "attractive\n",
      "wants\n",
      "both\n",
      "instructor\n",
      "those\n",
      "will\n",
      "near\n",
      "non\n",
      "middle\n",
      "funny\n",
      "different\n",
      "make\n",
      "same\n",
      "pick\n",
      "hang\n",
      "off\n",
      "well\n",
      "anybody\n",
      "bottle\n",
      "hubby\n",
      "musical\n",
      "outdoor\n",
      "athletic\n",
      "aunty\n",
      "executive\n",
      "hiking\n",
      "seems\n",
      "relatives\n",
      "adventurous\n",
      "location\n",
      "take\n",
      "checking\n",
      "advocate\n",
      "fremont\n",
      "early\n",
      "mon\n",
      "buddies\n",
      "either\n",
      "always\n",
      "hmmmm\n",
      "snuggle\n",
      "business\n",
      "kearny\n",
      "about\n",
      "anything\n",
      "dinner\n",
      "wanna\n",
      "road\n",
      "own\n",
      "unwind\n",
      "weirdos\n",
      "appropriate\n",
      "katie\n",
      "her\n",
      "lonely\n",
      "bored\n",
      "was\n",
      "happy\n",
      "media\n",
      "buy\n",
      "strictly\n",
      "trying\n",
      "places\n",
      "attached\n",
      "similar\n",
      "skating\n",
      "pic\n",
      "festival\n",
      "chat\n",
      "eastern\n",
      "other\n",
      "nice\n",
      "bottlerock\n",
      "dog\n",
      "far\n",
      "serious\n",
      "saying\n",
      "thoughtful\n",
      "asian\n",
      "trampling\n",
      "apartment\n",
      "buddy\n",
      "masculine\n",
      "stable\n",
      "sorry\n",
      "must\n",
      "woman\n",
      "flushing\n",
      "fat\n",
      "every\n",
      "drinking\n",
      "entire\n",
      "cause\n",
      "brother\n",
      "try\n",
      "sane\n",
      "round\n",
      "ten\n",
      "boots\n",
      "likely\n",
      "pass\n",
      "even\n",
      "what\n",
      "giving\n",
      "supporter\n",
      "international\n",
      "exchange\n",
      "men\n",
      "active\n",
      "sexy\n",
      "100\n",
      "strong\n",
      "personable\n",
      "fantasies\n",
      "kids\n",
      "daughter\n",
      "leaves\n",
      "alaska\n",
      "bow\n",
      "stressed\n",
      "prefer\n",
      "private\n",
      "rican\n",
      "cosplayer\n",
      "confidence\n",
      "from\n",
      "live\n",
      "symphony\n",
      "type\n",
      "tell\n",
      "more\n",
      "females\n",
      "knows\n",
      "occurred\n",
      "phone\n",
      "particular\n",
      "lifestyle\n",
      "mounted\n",
      "keeping\n",
      "room\n",
      "movies\n",
      "cuddle\n",
      "9th\n",
      "fulfill\n",
      "fakie\n",
      "trump\n",
      "give\n",
      "slim\n",
      "something\n",
      "keep\n",
      "information\n",
      "str8\n",
      "thing\n",
      "everyday\n",
      "nyc\n",
      "intelligent\n",
      "designer\n",
      "may\n",
      "advice\n",
      "wrong\n",
      "date\n",
      "suck\n",
      "guys\n",
      "cuddling\n",
      "light\n",
      "roommates\n",
      "enter\n",
      "tall\n",
      "furnished\n",
      "over\n",
      "move\n",
      "soon\n",
      "capatible\n",
      "whilst\n",
      "through\n",
      "looks\n",
      "its\n",
      "thrilling\n",
      "thank\n",
      "willing\n",
      "puerto\n",
      "bout\n",
      "criminal\n",
      "dad\n",
      "wouldn\n",
      "coffee\n",
      "seeking\n",
      "break\n",
      "front\n",
      "now\n",
      "fully\n",
      "realistic\n",
      "truth\n",
      "organized\n",
      "friendship\n",
      "side\n",
      "everyone\n",
      "house\n",
      "morning\n",
      "missed\n",
      "blk\n",
      "issue\n",
      "caucasian\n",
      "drink\n",
      "dirt\n",
      "guess\n",
      "free\n",
      "encountering\n",
      "besides\n",
      "ask\n",
      "laughable\n",
      "could\n",
      "conversation\n",
      "first\n",
      "dont\n",
      "feel\n",
      "done\n",
      "chubby\n",
      "plastic\n",
      "little\n",
      "willamsburg\n",
      "heights\n",
      "anyone\n",
      "their\n",
      "listen\n",
      "hangout\n",
      "massages\n",
      "part\n",
      "than\n",
      "depressed\n",
      "camilia\n",
      "midtown\n",
      "loves\n",
      "maybe\n",
      "lick\n",
      "san\n",
      "obviously\n",
      "built\n",
      "also\n",
      "ideal\n",
      "subjects\n",
      "latina\n",
      "fetish\n",
      "equestrian\n",
      "normal\n",
      "visitors\n",
      "brooklyn\n",
      "pair\n",
      "alpha\n",
      "why\n",
      "queens\n",
      "disease\n",
      "clean\n",
      "grovelling\n",
      "show\n",
      "supported\n",
      "bring\n",
      "earth\n",
      "guidance\n",
      "dick\n",
      "hopeful\n",
      "state\n",
      "being\n",
      "riding\n",
      "local\n",
      "hope\n",
      "plate\n",
      "hit\n",
      "watching\n",
      "ones\n",
      "actions\n",
      "him\n",
      "bad\n",
      "discussing\n",
      "beethoven\n",
      "sensuality\n",
      "humiliation\n",
      "sex\n",
      "see\n",
      "decided\n",
      "college\n",
      "visiting\n",
      "pictures\n",
      "hopefully\n",
      "hold\n",
      "reading\n",
      "frustrated\n",
      "available\n",
      "never\n",
      "recently\n",
      "condo\n",
      "cop\n",
      "taking\n",
      "adventures\n",
      "drug\n",
      "connection\n",
      "lookin\n",
      "asked\n",
      "league\n",
      "had\n",
      "makin\n",
      "trust\n",
      "conference\n",
      "companionship\n",
      "treat\n",
      "nudity\n",
      "flexible\n",
      "meeting\n",
      "affectionate\n",
      "life\n",
      "educated\n",
      "concerning\n",
      "search\n",
      "else\n",
      "doesn\n",
      "rubs\n",
      "sound\n",
      "myself\n",
      "gritty\n",
      "look\n",
      "upfont\n",
      "situation\n",
      "hoping\n",
      "then\n",
      "defeated\n",
      "ivy\n",
      "lacking\n",
      "suggest\n",
      "craigslist\n",
      "party\n",
      "couple\n",
      "week\n",
      "assignment\n",
      "husbands\n",
      "moment\n",
      "student\n",
      "identity\n",
      "older\n",
      "persuasive\n",
      "finds\n",
      "thought\n",
      "person\n",
      "drin\n",
      "inability\n",
      "position\n",
      "things\n",
      "kiss\n",
      "rest\n",
      "shape\n",
      "mannered\n",
      "cum\n",
      "real\n",
      "submissive\n",
      "around\n",
      "presentable\n",
      "honest\n",
      "possible\n",
      "listening\n",
      "world\n",
      "bit\n",
      "desire\n",
      "piont\n",
      "reduced\n",
      "because\n",
      "verbal\n",
      "30s\n",
      "donald\n",
      "intere\n",
      "drop\n",
      "though\n",
      "ice\n",
      "creative\n",
      "passion\n",
      "420\n",
      "desiring\n",
      "step\n",
      "concealed\n",
      "post\n",
      "super\n",
      "actual\n",
      "working\n",
      "most\n",
      "plus\n",
      "introduced\n",
      "ladies\n",
      "visit\n",
      "into\n",
      "errands\n",
      "articulate\n",
      "son\n",
      "down\n",
      "story\n",
      "deal\n",
      "confident\n",
      "housing\n",
      "accountability\n",
      "forward\n",
      "offering\n",
      "offer\n",
      "crown\n",
      "regard\n",
      "hear\n",
      "desings\n",
      "true\n",
      "handsome\n",
      "traveled\n",
      "made\n",
      "excitement\n",
      "problem\n",
      "called\n",
      "int\n",
      "certain\n",
      "moved\n",
      "muscular\n",
      "intro\n",
      "together\n",
      "girlfriend\n",
      "when\n",
      "reality\n",
      "corny\n",
      "manhattan\n",
      "chemist\n",
      "francisco\n",
      "students\n",
      "problems\n",
      "flatbush\n",
      "younger\n",
      "lead\n",
      "wife\n",
      "age\n",
      "daily\n",
      "classical\n",
      "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\n",
      "boots\n",
      "from\n",
      "will\n",
      "great\n",
      "free\n",
      "clean\n",
      "confidence\n",
      "live\n",
      "room\n",
      "want\n",
      "coffee\n",
      "house\n",
      "year\n",
      "drink\n",
      "could\n",
      "think\n",
      "midtown\n",
      "lick\n",
      "need\n",
      "any\n",
      "queens\n",
      "don\n",
      "find\n",
      "riding\n",
      "his\n",
      "please\n",
      "wants\n",
      "myself\n",
      "look\n",
      "straight\n",
      "hang\n",
      "well\n",
      "location\n",
      "submissive\n",
      "old\n",
      "step\n",
      "dinner\n",
      "down\n",
      "made\n",
      "pic\n",
      "moved\n",
      "chat\n",
      "nice\n",
      "students\n",
      "time\n",
      "asian\n",
      "trampling\n",
      "apartment\n",
      "buddy\n",
      "sorry\n",
      "flushing\n",
      "every\n",
      "drinking\n",
      "cool\n",
      "cause\n",
      "round\n",
      "what\n",
      "giving\n",
      "international\n",
      "exchange\n",
      "here\n",
      "explore\n",
      "let\n",
      "active\n",
      "strong\n",
      "kids\n",
      "daughter\n",
      "leaves\n",
      "alaska\n",
      "experience\n",
      "bow\n",
      "prefer\n",
      "few\n",
      "females\n",
      "getting\n",
      "lifestyle\n",
      "mounted\n",
      "keeping\n",
      "join\n",
      "work\n",
      "movies\n",
      "give\n",
      "slim\n",
      "something\n",
      "keep\n",
      "needs\n",
      "get\n",
      "nyc\n",
      "may\n",
      "advice\n",
      "suck\n",
      "cuddling\n",
      "light\n",
      "tall\n",
      "over\n",
      "whilst\n",
      "through\n",
      "bout\n",
      "wouldn\n",
      "good\n",
      "seeking\n",
      "safe\n",
      "break\n",
      "front\n",
      "day\n",
      "organized\n",
      "side\n",
      "mean\n",
      "girl\n",
      "morning\n",
      "missed\n",
      "caucasian\n",
      "dirt\n",
      "area\n",
      "days\n",
      "first\n",
      "platonic\n",
      "dont\n",
      "done\n",
      "chubby\n",
      "little\n",
      "anyone\n",
      "white\n",
      "massages\n",
      "part\n",
      "boring\n",
      "loves\n",
      "maybe\n",
      "obviously\n",
      "built\n",
      "also\n",
      "ideal\n",
      "latina\n",
      "fetish\n",
      "equestrian\n",
      "visitors\n",
      "brooklyn\n",
      "pair\n",
      "professional\n",
      "grovelling\n",
      "show\n",
      "earth\n",
      "busy\n",
      "only\n",
      "plate\n",
      "hit\n",
      "watching\n",
      "actions\n",
      "humiliation\n",
      "sex\n",
      "college\n",
      "visiting\n",
      "pictures\n",
      "available\n",
      "recently\n",
      "condo\n",
      "cop\n",
      "taking\n",
      "adventures\n",
      "connection\n",
      "asked\n",
      "alone\n",
      "trust\n",
      "conference\n",
      "treat\n",
      "nudity\n",
      "both\n",
      "concerning\n",
      "sound\n",
      "gritty\n",
      "suggest\n",
      "week\n",
      "husbands\n",
      "student\n",
      "off\n",
      "finds\n",
      "thought\n",
      "person\n",
      "drin\n",
      "inability\n",
      "things\n",
      "kiss\n",
      "not\n",
      "shape\n",
      "take\n",
      "honest\n",
      "bit\n",
      "either\n",
      "because\n",
      "verbal\n",
      "back\n",
      "ice\n",
      "post\n",
      "about\n",
      "actual\n",
      "anything\n",
      "ladies\n",
      "visit\n",
      "errands\n",
      "her\n",
      "housing\n",
      "hey\n",
      "forward\n",
      "happy\n",
      "offer\n",
      "trying\n",
      "places\n",
      "called\n",
      "int\n",
      "certain\n",
      "single\n",
      "intro\n",
      "corny\n",
      "flatbush\n",
      "friends\n",
      "wife\n",
      "serious\n",
      "hello\n",
      "all\n",
      "saying\n",
      "month\n",
      "thoughtful\n",
      "skate\n",
      "sleep\n",
      "prop\n",
      "young\n",
      "send\n",
      "masculine\n",
      "stable\n",
      "must\n",
      "woman\n",
      "advantage\n",
      "sitting\n",
      "very\n",
      "fat\n",
      "minded\n",
      "bestie\n",
      "entire\n",
      "skateboarder\n",
      "brother\n",
      "cloudy\n",
      "try\n",
      "sane\n",
      "enjoy\n",
      "ten\n",
      "tired\n",
      "likely\n",
      "traveling\n",
      "pass\n",
      "drinks\n",
      "even\n",
      "supporter\n",
      "outgoing\n",
      "full\n",
      "men\n",
      "healing\n",
      "making\n",
      "sexy\n",
      "100\n",
      "personable\n",
      "fantasies\n",
      "museums\n",
      "smoke\n",
      "stressed\n",
      "massage\n",
      "family\n",
      "private\n",
      "rican\n",
      "cosplayer\n",
      "dude\n",
      "symphony\n",
      "music\n",
      "wondering\n",
      "type\n",
      "tell\n",
      "more\n",
      "knows\n",
      "share\n",
      "company\n",
      "occurred\n",
      "phone\n",
      "particular\n",
      "effort\n",
      "women\n",
      "town\n",
      "cuddle\n",
      "9th\n",
      "fulfill\n",
      "fakie\n",
      "performed\n",
      "trump\n",
      "india\n",
      "spa\n",
      "information\n",
      "str8\n",
      "thing\n",
      "travel\n",
      "everyday\n",
      "how\n",
      "sunday\n",
      "intelligent\n",
      "designer\n",
      "after\n",
      "wrong\n",
      "birthday\n",
      "coming\n",
      "date\n",
      "guys\n",
      "types\n",
      "man\n",
      "short\n",
      "moments\n",
      "host\n",
      "roommates\n",
      "enter\n",
      "talk\n",
      "jack\n",
      "help\n",
      "furnished\n",
      "move\n",
      "soon\n",
      "capatible\n",
      "years\n",
      "looks\n",
      "its\n",
      "thrilling\n",
      "thank\n",
      "fit\n",
      "interesting\n",
      "clubbing\n",
      "willing\n",
      "puerto\n",
      "criminal\n",
      "dad\n",
      "they\n",
      "now\n",
      "sucked\n",
      "term\n",
      "name\n",
      "fully\n",
      "sleepless\n",
      "realistic\n",
      "40s\n",
      "truth\n",
      "friendship\n",
      "status\n",
      "harm\n",
      "everyone\n",
      "marital\n",
      "saturday\n",
      "living\n",
      "blk\n",
      "got\n",
      "receiving\n",
      "issue\n",
      "ass\n",
      "turning\n",
      "guess\n",
      "geared\n",
      "attending\n",
      "encountering\n",
      "besides\n",
      "ask\n",
      "laughable\n",
      "where\n",
      "skateboard\n",
      "desi\n",
      "times\n",
      "conversation\n",
      "place\n",
      "isn\n",
      "already\n",
      "feel\n",
      "one\n",
      "soci\n",
      "another\n",
      "plastic\n",
      "open\n",
      "city\n",
      "willamsburg\n",
      "slept\n",
      "girls\n",
      "least\n",
      "heights\n",
      "their\n",
      "thoughts\n",
      "too\n",
      "listen\n",
      "interests\n",
      "hangout\n",
      "relationship\n",
      "than\n",
      "depressed\n",
      "camilia\n",
      "project\n",
      "cabin\n",
      "san\n",
      "sac\n",
      "saf\n",
      "say\n",
      "nights\n",
      "subjects\n",
      "2016\n",
      "towards\n",
      "normal\n",
      "visa\n",
      "alpha\n",
      "why\n",
      "disease\n",
      "artistic\n",
      "meetings\n",
      "supported\n",
      "bring\n",
      "215\n",
      "bedroom\n",
      "guidance\n",
      "dick\n",
      "hopeful\n",
      "state\n",
      "being\n",
      "going\n",
      "pretty\n",
      "equally\n",
      "local\n",
      "hope\n",
      "dependent\n",
      "ones\n",
      "him\n",
      "married\n",
      "bay\n",
      "bad\n",
      "grab\n",
      "discussing\n",
      "geary\n",
      "beethoven\n",
      "sensuality\n",
      "see\n",
      "decided\n",
      "skateboarding\n",
      "close\n",
      "best\n",
      "camping\n",
      "enough\n",
      "hopefully\n",
      "hold\n",
      "reading\n",
      "frustrated\n",
      "email\n",
      "never\n",
      "complementary\n",
      "attention\n",
      "group\n",
      "drug\n",
      "etc\n",
      "lookin\n",
      "league\n",
      "supply\n",
      "had\n",
      "makin\n",
      "likes\n",
      "learning\n",
      "boat\n",
      "companionship\n",
      "decent\n",
      "been\n",
      "whom\n",
      "flexible\n",
      "meeting\n",
      "attractive\n",
      "affectionate\n",
      "life\n",
      "educated\n",
      "gay\n",
      "search\n",
      "else\n",
      "doesn\n",
      "rubs\n",
      "instructor\n",
      "those\n",
      "chill\n",
      "upfont\n",
      "near\n",
      "fun\n",
      "situation\n",
      "hoping\n",
      "then\n",
      "non\n",
      "defeated\n",
      "middle\n",
      "ivy\n",
      "funny\n",
      "different\n",
      "lacking\n",
      "make\n",
      "same\n",
      "craigslist\n",
      "party\n",
      "couple\n",
      "assignment\n",
      "pick\n",
      "moment\n",
      "identity\n",
      "older\n",
      "persuasive\n",
      "anybody\n",
      "bottle\n",
      "position\n",
      "hubby\n",
      "musical\n",
      "know\n",
      "outdoor\n",
      "athletic\n",
      "aunty\n",
      "executive\n",
      "rest\n",
      "hiking\n",
      "seems\n",
      "relatives\n",
      "adventurous\n",
      "interested\n",
      "mannered\n",
      "cum\n",
      "has\n",
      "smart\n",
      "real\n",
      "around\n",
      "presentable\n",
      "checking\n",
      "advocate\n",
      "fremont\n",
      "possible\n",
      "early\n",
      "discreet\n",
      "mon\n",
      "listening\n",
      "world\n",
      "lady\n",
      "desire\n",
      "buddies\n",
      "always\n",
      "piont\n",
      "reduced\n",
      "people\n",
      "30s\n",
      "donald\n",
      "intere\n",
      "drop\n",
      "hmmmm\n",
      "though\n",
      "creative\n",
      "passion\n",
      "420\n",
      "desiring\n",
      "snuggle\n",
      "business\n",
      "kearny\n",
      "concealed\n",
      "super\n",
      "working\n",
      "most\n",
      "plus\n",
      "introduced\n",
      "wanna\n",
      "road\n",
      "own\n",
      "unwind\n",
      "into\n",
      "weirdos\n",
      "articulate\n",
      "son\n",
      "appropriate\n",
      "katie\n",
      "story\n",
      "deal\n",
      "confident\n",
      "accountability\n",
      "lonely\n",
      "bored\n",
      "was\n",
      "media\n",
      "buy\n",
      "offering\n",
      "crown\n",
      "regard\n",
      "strictly\n",
      "hear\n",
      "desings\n",
      "true\n",
      "handsome\n",
      "traveled\n",
      "attached\n",
      "excitement\n",
      "problem\n",
      "similar\n",
      "skating\n",
      "muscular\n",
      "festival\n",
      "together\n",
      "home\n",
      "girlfriend\n",
      "eastern\n",
      "when\n",
      "reality\n",
      "other\n",
      "manhattan\n",
      "chemist\n",
      "francisco\n",
      "bottlerock\n",
      "problems\n",
      "younger\n",
      "lead\n",
      "age\n",
      "dog\n",
      "daily\n",
      "far\n",
      "classical\n"
     ]
    }
   ],
   "source": [
    "bayes.getTopWords(ny, sf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
